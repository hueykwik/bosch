{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:41:58.587716",
     "start_time": "2016-10-31T14:41:58.581622"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_union\n",
    "from numpy import ravel\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:41:59.511849",
     "start_time": "2016-10-31T14:41:59.477204"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: Use the TransformerMixin classes below to create features and then use FeatureUnions\n",
    "# to concatenate them together.\n",
    "\n",
    "# Example transformer to take a data frame and then return some features via the transform()\n",
    "# method.\n",
    "class Regular(TransformerMixin):\n",
    "    #return itself, use this for when you have a dataframe already in the script base\n",
    "    def __init__(self, df):\n",
    "        # TODO build in args so that we can pass things to read_csv\n",
    "        self.df = df\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.df\n",
    "\n",
    "# Loads a CSV and then returns a set of features.\n",
    "class LoadCSV(TransformerMixin):\n",
    "    # use this to load in an external csv, it accepts kwargs to feed to pd.read_csv()\n",
    "    def __init__(self, filename, **kwargs):\n",
    "        # TODO build in args so that we can pass things to read_csv\n",
    "        self.filename = filename\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # we assume the first CSV entry is always the ID\n",
    "        temp = pd.read_csv(self.filename, index_col = 0, **self.kwargs)\n",
    "        return temp\n",
    "\n",
    "# Loads from a directory in chunks and then returns a dataframe.\n",
    "def load_data(directory, files, cols):\n",
    "    # Huey's older loading method, should deprecate this I think\n",
    "    df = None\n",
    "    for i, f in enumerate(files):\n",
    "        print(f)\n",
    "        subset = None\n",
    "        \n",
    "        for i, chunk in enumerate(pd.read_csv(directory + f,\n",
    "                                              usecols=cols[i],\n",
    "                                              chunksize=50000,\n",
    "                                              low_memory=False)):\n",
    "            if i % 5 == 0:\n",
    "                print('Processing chunk %d' % i)\n",
    "            if subset is None:\n",
    "                subset = chunk.copy()\n",
    "            else:\n",
    "                subset = pd.concat([subset, chunk])\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "            \n",
    "        if df is None:\n",
    "            df = subset.copy()\n",
    "        else:\n",
    "            df = pd.merge(df, subset.copy(), on=\"Id\")\n",
    "        del subset\n",
    "        gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:42:18.328827",
     "start_time": "2016-10-31T14:42:00.796302"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load in targets\n",
    "base = pd.read_csv(\"data/train_numeric.csv\", usecols = ['Id','Response'])\n",
    "target = base.loc[:,['Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:43:04.602897",
     "start_time": "2016-10-31T14:43:04.593824"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files = ['train_date.csv',\n",
    "              'train_numeric.csv']\n",
    "\n",
    "test_files = ['test_date.csv',\n",
    "             'test_numeric.csv']\n",
    "\n",
    "train_cols = [\n",
    "        ['Id',\n",
    "         'L3_S30_D3496', 'L3_S30_D3506',\n",
    "         'L3_S30_D3501', 'L3_S30_D3516',\n",
    "         'L3_S30_D3511'],\n",
    "        ['Id',\n",
    "         'L1_S24_F1846', 'L3_S32_F3850',\n",
    "         'L1_S24_F1695', 'L1_S24_F1632',\n",
    "         'L3_S33_F3855', 'L1_S24_F1604',\n",
    "         'L3_S29_F3407', 'L3_S33_F3865',\n",
    "         'L3_S38_F3952', 'L1_S24_F1723',\n",
    "         'Response'],\n",
    "        ['Id','Fail']]\n",
    "\n",
    "test_cols = [\n",
    "        ['Id',\n",
    "         'L3_S30_D3496', 'L3_S30_D3506',\n",
    "         'L3_S30_D3501', 'L3_S30_D3516',\n",
    "         'L3_S30_D3511'],\n",
    "        ['Id',\n",
    "         'L1_S24_F1846', 'L3_S32_F3850',\n",
    "         'L1_S24_F1695', 'L1_S24_F1632',\n",
    "         'L3_S33_F3855', 'L1_S24_F1604',\n",
    "         'L3_S29_F3407', 'L3_S33_F3865',\n",
    "         'L3_S38_F3952', 'L1_S24_F1723'],\n",
    "        ['Id','Fail']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:44:17.629174",
     "start_time": "2016-10-31T14:43:08.050080"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_date.csv\n",
      "Processing chunk 0\n",
      "Processing chunk 5\n",
      "Processing chunk 10\n",
      "Processing chunk 15\n",
      "Processing chunk 20\n",
      "train_numeric.csv\n",
      "Processing chunk 0\n",
      "Processing chunk 5\n",
      "Processing chunk 10\n",
      "Processing chunk 15\n",
      "Processing chunk 20\n",
      "(1183747, 17)\n"
     ]
    }
   ],
   "source": [
    "train_raw_features = load_data('data/', train_files, train_cols)\n",
    "print(train_raw_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:46:29.893285",
     "start_time": "2016-10-31T14:45:19.500828"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_date.csv\n",
      "Processing chunk 0\n",
      "Processing chunk 5\n",
      "Processing chunk 10\n",
      "Processing chunk 15\n",
      "Processing chunk 20\n",
      "test_numeric.csv\n",
      "Processing chunk 0\n",
      "Processing chunk 5\n",
      "Processing chunk 10\n",
      "Processing chunk 15\n",
      "Processing chunk 20\n",
      "(1183748, 16)\n"
     ]
    }
   ],
   "source": [
    "test_raw_features = load_data('data/', test_files, test_cols)\n",
    "print(test_raw_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:46:29.913839",
     "start_time": "2016-10-31T14:46:29.895307"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Id', u'L3_S30_D3496', u'L3_S30_D3501', u'L3_S30_D3506',\n",
       "       u'L3_S30_D3511', u'L3_S30_D3516', u'L1_S24_F1604', u'L1_S24_F1632',\n",
       "       u'L1_S24_F1695', u'L1_S24_F1723', u'L1_S24_F1846', u'L3_S29_F3407',\n",
       "       u'L3_S32_F3850', u'L3_S33_F3855', u'L3_S33_F3865', u'L3_S38_F3952'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_rawfeatures = train_mindate\n",
    "#test_rawfeatures = test_mindate\n",
    "test_raw_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:46:30.606974",
     "start_time": "2016-10-31T14:46:29.916087"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove Id and Response\n",
    "train_raw_features = train_raw_features[train_raw_features.columns.difference(['Id', 'Response'])]\n",
    "test_raw_features = test_raw_features[test_raw_features.columns.difference(['Id', 'Response'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:47:14.595796",
     "start_time": "2016-10-31T14:47:12.576006"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make FeatureUnions\n",
    "# load in each piece of data from 'data/' that you want\n",
    "features = make_union(LoadCSV('data/train_fail_date_score.csv'), \n",
    "                          # Counts the number of failures that \n",
    "                          # happened between the first and last datetime.\n",
    "                      LoadCSV('data/train_min_date.csv'),\n",
    "                          # This is the so-called \"magic\" feature.\n",
    "                      LoadCSV('data/train_s32_s33_s34.csv'),\n",
    "                          # Indicates whether part passed through S32, S33, or S34.\n",
    "                      Regular(train_raw_features))\n",
    "\n",
    "#                      LoadCSV('data/train_id_rates_max.csv'),\n",
    "#                      LoadCSV('data/train_id_rates_total.csv'))\n",
    "# mindate's best columns, Huey's (is S32, S33, S34), \n",
    "# nathan's cyclic thing, fail_date_score, useful date columns\n",
    "\n",
    "\n",
    "X = features.fit_transform(1)\n",
    "y = base['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:48:06.474605",
     "start_time": "2016-10-31T14:48:04.483580"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = make_union(LoadCSV('data/test_fail_date_score.csv'), \n",
    "                  LoadCSV('data/test_min_date.csv'),\n",
    "                  LoadCSV('data/test_s32_s33_s34.csv'),\n",
    "                  Regular(test_raw_features))\n",
    "#                  LoadCSV('data/test_id_rates_max.csv'),\n",
    "#                  LoadCSV('data/test_id_rates_total.csv'))\n",
    "X_test = test.fit_transform(1)\n",
    "df_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:48:08.539918",
     "start_time": "2016-10-31T14:48:08.535894"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if X.shape[1] != X_test.shape[1]:\n",
    "    print('loaded in CSVs wrong')\n",
    "\n",
    "#clf = XGBClassifier(base_score=0.005, seed=24)\n",
    "#clf.fit(X,y)\n",
    "##original_preds = np.ones(y.shape[0])\n",
    "#original_raw_preds = clf.predict_proba(X)\n",
    "#original_preds = (clf.predict_proba(X)[:,1] > 0.05).astype(np.int8)\n",
    "#newpreds = (clf.predict_proba(test)[:,1] > 0.05).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:55:28.544524",
     "start_time": "2016-10-31T14:48:10.124901"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0, ROC AUC: 0.910\n",
      "fold 1, ROC AUC: 0.902\n",
      "fold 2, ROC AUC: 0.894\n",
      "0.901577240223\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(max_depth=5, base_score=0.005, seed=37)\n",
    "cv = StratifiedKFold(y, n_folds=3, random_state=37)\n",
    "preds = np.ones(y.shape[0])\n",
    "dfX = pd.DataFrame(X)\n",
    "\n",
    "for i, (infold, outfold) in enumerate(cv):\n",
    "    preds[outfold] = clf.fit(dfX.loc[infold], y[infold]).predict_proba(dfX.loc[outfold])[:,1]\n",
    "    print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(y[outfold], preds[outfold])))\n",
    "print(roc_auc_score(y, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:58:33.571296",
     "start_time": "2016-10-31T14:58:19.763996"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.387231789121\n"
     ]
    }
   ],
   "source": [
    "# Pick the best threshold out-of-fold\n",
    "thresholds = np.linspace(0.01, 0.99, 50)\n",
    "mcc = np.array([matthews_corrcoef(y, preds>thr) for thr in thresholds])\n",
    "plt.plot(thresholds, mcc)\n",
    "best_threshold = thresholds[mcc.argmax()]\n",
    "print(mcc.max())\n",
    "preds = (clf.predict_proba(df_test)[:,1] > best_threshold).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-31T14:58:46.548138",
     "start_time": "2016-10-31T14:58:33.573392"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Submit\n",
    "sub = pd.read_csv(\"data/sample_submission.csv\", index_col=0)\n",
    "sub[\"Response\"] = preds\n",
    "sub.to_csv(\"pipesubmission.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
