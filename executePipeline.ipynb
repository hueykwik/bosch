{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T16:58:10.014418",
     "start_time": "2016-10-30T16:58:07.809556"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_union\n",
    "from numpy import ravel\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T16:58:10.080594",
     "start_time": "2016-10-30T16:58:10.015421"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# contains all methods and classes for FeatureUnion\n",
    "## generic read_csv\n",
    "\n",
    "#example\n",
    "class Regular(TransformerMixin):\n",
    "    #return itself, use this for when you have a dataframe already in the script base\n",
    "    def __init__(self, df):\n",
    "        # TODO build in args so that we can pass things to read_csv\n",
    "        self.df = df\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.df\n",
    "\n",
    "#example\n",
    "class LoadCSV(TransformerMixin):\n",
    "    # use this to load in an external csv, it accepts kwargs to feed to pd.read_csv()\n",
    "    def __init__(self, filename, **kwargs):\n",
    "        # TODO build in args so that we can pass things to read_csv\n",
    "        self.filename = filename\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # we assume the first CSV entry is always the ID\n",
    "        temp = pd.read_csv(self.filename, index_col = 0, **self.kwargs)\n",
    "        return temp\n",
    "\n",
    "def load_data(directory, files, cols):\n",
    "    # Huey's older loading method, should deprecate this I think\n",
    "    df = None\n",
    "    for i, f in enumerate(files):\n",
    "        print(f)\n",
    "        subset = None\n",
    "        \n",
    "        for i, chunk in enumerate(pd.read_csv(directory + f,\n",
    "                                              usecols=cols[i],\n",
    "                                              chunksize=50000,\n",
    "                                              low_memory=False)):\n",
    "            if i % 5 == 0:\n",
    "                print('Processing chunk %d' % i)\n",
    "            if subset is None:\n",
    "                subset = chunk.copy()\n",
    "            else:\n",
    "                subset = pd.concat([subset, chunk])\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "            \n",
    "        if df is None:\n",
    "            df = subset.copy()\n",
    "        else:\n",
    "            df = pd.merge(df, subset.copy(), on=\"Id\")\n",
    "        del subset\n",
    "        gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T16:58:26.113767",
     "start_time": "2016-10-30T16:58:10.082600"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load in targets\n",
    "base = pd.read_csv(\"data/train_numeric.csv\", usecols = ['Id','Response'])\n",
    "target = base.loc[:,['Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T16:58:26.122383",
     "start_time": "2016-10-30T16:58:26.115364"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files = ['train_date.csv',\n",
    "              'train_numeric.csv']\n",
    "\n",
    "test_files = ['test_date.csv',\n",
    "             'test_numeric.csv']\n",
    "\n",
    "train_cols = [\n",
    "        ['Id',\n",
    "         'L3_S30_D3496', 'L3_S30_D3506',\n",
    "         'L3_S30_D3501', 'L3_S30_D3516',\n",
    "         'L3_S30_D3511'],\n",
    "        ['Id',\n",
    "         'L1_S24_F1846', 'L3_S32_F3850',\n",
    "         'L1_S24_F1695', 'L1_S24_F1632',\n",
    "         'L3_S33_F3855', 'L1_S24_F1604',\n",
    "         'L3_S29_F3407', 'L3_S33_F3865',\n",
    "         'L3_S38_F3952', 'L1_S24_F1723',\n",
    "         'Response'],\n",
    "        ['Id','Fail']]\n",
    "\n",
    "test_cols = [\n",
    "        ['Id',\n",
    "         'L3_S30_D3496', 'L3_S30_D3506',\n",
    "         'L3_S30_D3501', 'L3_S30_D3516',\n",
    "         'L3_S30_D3511'],\n",
    "        ['Id',\n",
    "         'L1_S24_F1846', 'L3_S32_F3850',\n",
    "         'L1_S24_F1695', 'L1_S24_F1632',\n",
    "         'L3_S33_F3855', 'L1_S24_F1604',\n",
    "         'L3_S29_F3407', 'L3_S33_F3865',\n",
    "         'L3_S38_F3952', 'L1_S24_F1723'],\n",
    "        ['Id','Fail']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T16:59:44.194964",
     "start_time": "2016-10-30T16:58:26.123388"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_date.csv\n",
      "Processing chunk 0\n",
      "Processing chunk 5\n",
      "Processing chunk 10\n",
      "Processing chunk 15\n",
      "Processing chunk 20\n",
      "train_numeric.csv\n",
      "Processing chunk 0\n",
      "Processing chunk 5\n",
      "Processing chunk 10\n",
      "Processing chunk 15\n",
      "Processing chunk 20\n",
      "(1183747, 17)\n"
     ]
    }
   ],
   "source": [
    "train_rawfeatures = load_data('data/', train_files, train_cols)\n",
    "print(train_rawfeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T17:01:02.935313",
     "start_time": "2016-10-30T16:59:44.195965"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_date.csv\n",
      "Processing chunk 0\n",
      "Processing chunk 5\n",
      "Processing chunk 10\n",
      "Processing chunk 15\n",
      "Processing chunk 20\n",
      "test_numeric.csv\n",
      "Processing chunk 0\n",
      "Processing chunk 5\n",
      "Processing chunk 10\n",
      "Processing chunk 15\n",
      "Processing chunk 20\n",
      "(1183748, 16)\n"
     ]
    }
   ],
   "source": [
    "test_rawfeatures = load_data('data/', test_files, test_cols)\n",
    "print(test_rawfeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T17:01:02.942334",
     "start_time": "2016-10-30T17:01:02.936315"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'L3_S30_D3496', 'L3_S30_D3501', 'L3_S30_D3506', 'L3_S30_D3511',\n",
       "       'L3_S30_D3516', 'L1_S24_F1604', 'L1_S24_F1632', 'L1_S24_F1695',\n",
       "       'L1_S24_F1723', 'L1_S24_F1846', 'L3_S29_F3407', 'L3_S32_F3850',\n",
       "       'L3_S33_F3855', 'L3_S33_F3865', 'L3_S38_F3952'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_rawfeatures = train_mindate\n",
    "#test_rawfeatures = test_mindate\n",
    "test_rawfeatures.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T17:01:03.536408",
     "start_time": "2016-10-30T17:01:02.945340"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove Id and Response\n",
    "train_rawfeatures = train_rawfeatures[train_rawfeatures.columns.difference(['Id', 'Response'])]\n",
    "test_rawfeatures = test_rawfeatures[test_rawfeatures.columns.difference(['Id', 'Response'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T17:01:09.948781",
     "start_time": "2016-10-30T17:01:05.996256"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make featureunions\n",
    "# load in each piece of data from 'data/' that you want\n",
    "features = make_union(LoadCSV('data/train_fail_date_score.csv'), \n",
    "                          #dave's first feature, counts the number of failures that \n",
    "                          #happened between the first and last datetime\n",
    "                      LoadCSV('data/train_min_date.csv'),\n",
    "                          # This is the leaked feature\n",
    "                      LoadCSV('data/train_s32_s33_s34.csv'),\n",
    "                          #Huey's dummy-style frame for tracking whether or not \n",
    "                          #the Id passed through stations\n",
    "                      Regular(train_rawfeatures))\n",
    "#                      LoadCSV('data/train_id_rates_max.csv'),\n",
    "#                      LoadCSV('data/train_id_rates_total.csv'))\n",
    "# mindate's best columns, Huey's (is S32, S33, S34), \n",
    "# nathan's cyclic thing, fail_date_score, useful date columns\n",
    "\n",
    "\n",
    "X = features.fit_transform(1)\n",
    "y = base['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T17:01:13.468819",
     "start_time": "2016-10-30T17:01:09.949783"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test = make_union(LoadCSV('data/test_fail_date_score.csv'), \n",
    "                  LoadCSV('data/test_min_date.csv'),\n",
    "                  LoadCSV('data/test_s32_s33_s34.csv'),\n",
    "                  Regular(test_rawfeatures))\n",
    "#                  LoadCSV('data/test_id_rates_max.csv'),\n",
    "#                  LoadCSV('data/test_id_rates_total.csv'))\n",
    "X_test = test.fit_transform(1)\n",
    "df_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T17:01:35.263382",
     "start_time": "2016-10-30T17:01:35.259370"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if X.shape[1] != X_test.shape[1]:\n",
    "    print('loaded in CSVs wrong')\n",
    "\n",
    "#clf = XGBClassifier(base_score=0.005, seed=24)\n",
    "#clf.fit(X,y)\n",
    "##original_preds = np.ones(y.shape[0])\n",
    "#original_raw_preds = clf.predict_proba(X)\n",
    "#original_preds = (clf.predict_proba(X)[:,1] > 0.05).astype(np.int8)\n",
    "#newpreds = (clf.predict_proba(test)[:,1] > 0.05).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T17:06:56.598399",
     "start_time": "2016-10-30T17:01:36.835187"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0, ROC AUC: 0.910\n",
      "fold 1, ROC AUC: 0.902\n",
      "fold 2, ROC AUC: 0.880\n",
      "0.755557974431\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(max_depth=5, base_score=0.005, seed=37)\n",
    "cv = StratifiedKFold(y, n_folds=3, random_state=37)\n",
    "preds = np.ones(y.shape[0])\n",
    "dfX = pd.DataFrame(X)\n",
    "\n",
    "for i, (infold, outfold) in enumerate(cv):\n",
    "    preds[outfold] = clf.fit(dfX.loc[infold], y[infold]).predict_proba(dfX.loc[outfold])[:,1]\n",
    "    print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(y[outfold], preds[outfold])))\n",
    "print(roc_auc_score(y, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T17:07:16.202931",
     "start_time": "2016-10-30T17:07:05.822148"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.206598279908\n"
     ]
    }
   ],
   "source": [
    "# Pick the best threshold out-of-fold\n",
    "thresholds = np.linspace(0.01, 0.99, 50)\n",
    "mcc = np.array([matthews_corrcoef(y, preds>thr) for thr in thresholds])\n",
    "plt.plot(thresholds, mcc)\n",
    "best_threshold = thresholds[mcc.argmax()]\n",
    "print(mcc.max())\n",
    "preds = (clf.predict_proba(df_test)[:,1] > best_threshold).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-30T17:07:21.711559",
     "start_time": "2016-10-30T17:07:16.203933"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Submit\n",
    "sub = pd.read_csv(\"data/sample_submission.csv\", index_col=0)\n",
    "sub[\"Response\"] = preds\n",
    "sub.to_csv(\"pipesubmission.csv.gz\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
