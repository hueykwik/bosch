{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T11:45:03.875679",
     "start_time": "2016-09-28T11:45:00.314387"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Configuration\n",
    "Here we set up Amazon keys and mount S3. You should only need to run this cell once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T12:38:19.221596",
     "start_time": "2016-09-28T12:38:19.217139"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"AKIAITZKMRPEM3K3AHQQ\"\n",
    "SECRET_KEY = \"wsO6AO2sn3bru8lwgwqa5spv0u2C3mdvRg4gTixL\"\n",
    "ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "AWS_BUCKET_NAME = \"daveandhuey\"\n",
    "MOUNT_NAME = \"daveandhuey\"\n",
    "\n",
    "#dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T11:45:03.904919",
     "start_time": "2016-09-28T11:45:03.887152"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://daveandhuey/train_numeric.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def s3_path(filename):\n",
    "    return 's3://%s/%s' % (AWS_BUCKET_NAME, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T11:45:03.913843",
     "start_time": "2016-09-28T11:45:03.906760"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If running locally, assume that all the numeric files are there\n",
    "train_num_path = 'data/train_numeric.csv'\n",
    "train_date_path = 'data/train_date.csv'\n",
    "\n",
    "test_num_path = 'data/test_numeric.csv'\n",
    "test_date_path = 'data/test_date.csv'\n",
    "\n",
    "data_file_exists = path.isdir('data')\n",
    "\n",
    "if data_file_exists == False:\n",
    "    train_num_path = s3_path('train_numeric.csv')\n",
    "    train_date_path = s3_path('train_date.csv')\n",
    "    \n",
    "    test_num_path = s3_path('test_numeric.csv')\n",
    "    test_date_path = s3_path('test_date.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Boto to grab the sample submission file from S3. For whatever reason, Pandas doesn't handle the mounted paths correctly, so we download the .csv as a string and then pass it into Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T12:38:20.519320",
     "start_time": "2016-09-28T12:38:20.296326"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3c130b9378be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_contents_as_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.connection import Key\n",
    "\n",
    "# Open a connection to S3.\n",
    "c = S3Connection(ACCESS_KEY, SECRET_KEY, host='s3-us-west-2.amazonaws.com')\n",
    "\n",
    "bucket = c.get_bucket(AWS_BUCKET_NAME)\n",
    "k = Key(bucket)\n",
    "k.key = 'sample_submission.csv'\n",
    "\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "  from StringIO import StringIO\n",
    "else:\n",
    "  from io import StringIO\n",
    "\n",
    "sub = pd.read_csv(StringIO(k.get_contents_as_string()), sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predicting\n",
    "Here we load the numeric and test data, train a GBT classifier, and then make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T11:46:40.193457",
     "start_time": "2016-09-28T11:45:09.581006"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Response: int, features: vector]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num = spark.read.csv(train_num_path, header=\"true\", inferSchema=\"true\")\n",
    "train_date = spark.read.csv(train_date_path, header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "# TODO(hkwik): Figure out when this is necessary. Code I've seen using XGBoost doesn't impute NaN.\n",
    "train_num = train_num.na.fill(0.0)  \n",
    "train_date = train_date.na.fill(0.0)\n",
    "\n",
    "train = train_num.join(train_date, train_num.Id == train_date.Id)\n",
    "\n",
    "ignore = ['Id', 'Response']\n",
    "lista=[x for x in train.columns if x not in ignore]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=lista, outputCol='features')\n",
    "\n",
    "train = (assembler.transform(train).select('Response', 'features'))\n",
    "\n",
    "## Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, validData) = train.randomSplit([0.7, 0.3], seed=24)\n",
    "trainingData.cache()\n",
    "validData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T11:56:38.403875",
     "start_time": "2016-09-28T11:46:40.195332"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a GBT model.\n",
    "# maxIter = 1 for speed of testing.\n",
    "gbt = GBTClassifier(labelCol=\"Response\", featuresCol=\"features\", maxIter=1, seed=24)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData) # Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T12:02:29.926247",
     "start_time": "2016-09-28T11:56:38.406166"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.127729172579 \n"
     ]
    }
   ],
   "source": [
    "# Test against validation set\n",
    "predictions = model.transform(validData)\n",
    "predsGBT = predictions.select(\"prediction\").rdd.map(lambda r: r[0]).collect()\n",
    "preds = np.asarray(predsGBT).astype(int)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "valid_responses = validData.select('response').rdd.map(lambda r: r[0]).collect()\n",
    "print('MCC: %s ' % matthews_corrcoef(valid_responses, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T12:09:39.459407",
     "start_time": "2016-09-28T12:02:29.928146"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions and submission frame\n",
    "test_num = spark.read.csv(test_num_path, header=\"true\", inferSchema=\"true\")\n",
    "test_date = spark.read.csv(test_date_path, header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "test_join = test_num.join(test_date, \"Id\")\n",
    "# L3_S46_D4135 looks like a bunch of empty strings, so it has a StringType.\n",
    "test_join = test_join.withColumn('L3_S46_D4135', test_join['L3_S46_D4135'].cast(DoubleType()))\n",
    "test_join = test_join.na.fill(0.0)\n",
    "\n",
    "test = (assembler.transform(test_join).select(\"features\"))\n",
    "\n",
    "predictions_test = model.transform(test)\n",
    "\n",
    "predsGBT = predictions_test.select(\"prediction\").rdd.map(lambda r: r[0]).collect() \n",
    "\n",
    "sub['Response'] = np.asarray(predsGBT).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Submission \n",
    "Write the submission Pandas DataFrame to our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T12:09:43.549833",
     "start_time": "2016-09-28T12:09:39.461093"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11281556"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_str = sub.to_csv(None, index=False)\n",
    "k = Key(bucket)\n",
    "k.key = 'bosch-submission.csv'\n",
    "k.set_contents_from_string(sub_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T12:51:00.387983",
     "start_time": "2016-09-28T12:51:00.374486"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0c0301af3e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "bosch_spark",
  "notebookId": 2204623898018108
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
