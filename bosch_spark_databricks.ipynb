{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T10:57:24.001075",
     "start_time": "2016-09-28T10:57:18.896580"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Configuration\n",
    "Here we set up Amazon keys and mount S3. You should only need to run this cell once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T10:57:26.159154",
     "start_time": "2016-09-28T10:57:25.787142"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1202452aa0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mMOUNT_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"daveandhuey\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://%s:%s@%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mACCESS_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENCODED_SECRET_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAWS_BUCKET_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/mnt/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mMOUNT_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "ACCESS_KEY = \"AKIAITZKMRPEM3K3AHQQ\"\n",
    "SECRET_KEY = \"wsO6AO2sn3bru8lwgwqa5spv0u2C3mdvRg4gTixL\"\n",
    "ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "AWS_BUCKET_NAME = \"daveandhuey\"\n",
    "MOUNT_NAME = \"daveandhuey\"\n",
    "\n",
    "dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Boto to grab the sample submission file from S3. For whatever reason, Pandas doesn't handle the mounted paths correctly, so we download the .csv as a string and then pass it into Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-28T11:02:53.953212",
     "start_time": "2016-09-28T11:02:50.119744"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.connection import Key\n",
    "\n",
    "# Open a connection to S3.\n",
    "c = S3Connection(ACCESS_KEY, SECRET_KEY)\n",
    "\n",
    "bucket = c.get_bucket(AWS_BUCKET_NAME)\n",
    "k = Key(bucket)\n",
    "k.key = 'sample_submission.csv'\n",
    "\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "  from StringIO import StringIO\n",
    "else:\n",
    "  from io import StringIO\n",
    "\n",
    "sub = pd.read_csv(StringIO(k.get_contents_as_string()), sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predicting\n",
    "Here we load the numeric and test data, train a GBT classifier, and then make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_num = spark.read.csv(\"/mnt/%s/train_numeric.csv\" % MOUNT_NAME, header=\"true\", inferSchema=\"true\")\n",
    "train_date = spark.read.csv(\"/mnt/%s/train_date.csv\" % MOUNT_NAME, header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "# TODO(hkwik): Figure out when this is necessary. Code I've seen using XGBoost doesn't impute NaN.\n",
    "train_num = train_num.na.fill(0.0)  \n",
    "train_date = train_date.na.fill(0.0)\n",
    "\n",
    "train = train_num.join(train_date, train_num.Id == train_date.Id)\n",
    "\n",
    "ignore = ['Id', 'Response']\n",
    "lista=[x for x in train.columns if x not in ignore]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=lista, outputCol='features')\n",
    "\n",
    "train = (assembler.transform(train).select('Response', 'features'))\n",
    "\n",
    "## Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, validData) = train.randomSplit([0.7, 0.3], seed=24)\n",
    "trainingData.cache()\n",
    "validData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a GBT model.\n",
    "# maxIter = 1 for speed of testing.\n",
    "gbt = GBTClassifier(labelCol=\"Response\", featuresCol=\"features\", maxIter=1, seed=24)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData) # Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test against validation set\n",
    "predictions = model.transform(validData)\n",
    "predsGBT = predictions.select(\"prediction\").rdd.map(lambda r: r[0]).collect()\n",
    "preds = np.asarray(predsGBT).astype(int)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "valid_responses = validData.select('response').rdd.map(lambda r: r[0]).collect()\n",
    "print('MCC: %s ' % matthews_corrcoef(valid_responses, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions and submission frame\n",
    "test_num = spark.read.csv(\"/mnt/%s/test_numeric.csv\" % MOUNT_NAME, header=\"true\", inferSchema=\"true\")\n",
    "test_date = spark.read.csv(\"/mnt/%s/test_date.csv\" % MOUNT_NAME, header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "test_join = test_num.join(test_date, \"Id\")\n",
    "# L3_S46_D4135 looks like a bunch of empty strings, so it has a StringType.\n",
    "test_join = test_join.withColumn('L3_S46_D4135', test_join['L3_S46_D4135'].cast(DoubleType()))\n",
    "test_join = test_join.na.fill(0.0)\n",
    "\n",
    "test = (assembler.transform(test_join).select(\"features\"))\n",
    "\n",
    "predictions_test = model.transform(test)\n",
    "\n",
    "predsGBT = predictions_test.select(\"prediction\").rdd.map(lambda r: r[0]).collect() \n",
    "\n",
    "sub['Response'] = np.asarray(predsGBT).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Submission \n",
    "Write the submission Pandas DataFrame to our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_str = sub.to_csv(None, index=False)\n",
    "k = Key(bucket)\n",
    "k.key = 'bosch-submission.csv'\n",
    "k.set_contents_from_string(sub_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want to use GridSearchCV\n",
    "from spark_sklearn import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "bosch_spark",
  "notebookId": 2204623898018108
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
